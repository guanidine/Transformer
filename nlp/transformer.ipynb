{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Transformer</h1> </center>\n",
    "\n",
    "<center><p><a href=\"https://arxiv.org/abs/1706.03762\">Attention is All You Need\n",
    "</a></p></center>\n",
    "\n",
    "<img src=\"https://production-media.paperswithcode.com/methods/new_ModalNet-21.jpg\" width=\"600\"/>\n",
    "\n",
    "[Code: https://github.com/harvardnlp/annotated-transformer/](https://github.com/harvardnlp/annotated-transformer/)\n",
    "\n",
    "[Code with annotation: http://nlp.seas.harvard.edu/annotated-transformer/](http://nlp.seas.harvard.edu/annotated-transformer/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input/Output Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab, d_model=512):\n",
    "        \"\"\"\n",
    "        Embeddings and Softmax.\n",
    "        :param vocab: dictionary size of the source vocabulary.\n",
    "        :param d_model: the size of each embedding vector. Default: 512\n",
    "        \"\"\"\n",
    "        super(Embedding, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Embeddings and Softmax.\n",
    "        :param x: (batch_size, seq_length)\n",
    "        :return: (batch_size, seq_length, d_model)\n",
    "        \"\"\"\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model=512, dropout=0.1, max_len=5000):\n",
    "        \"\"\"\n",
    "        Positional Encoding.\n",
    "        :param d_model: the size of each embedding vector. Default: 512\n",
    "        :param dropout: probability of an element to be zeroed. Default: 0.1\n",
    "        :param max_len: max length of the sequence. Default: 5000\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float()\n",
    "            * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Positional Encoding.\n",
    "        :param x: (batch_size, seq_length, d_model)\n",
    "        :return: (batch_size, seq_length, d_model)\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sublayers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h=8, d_model=512, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Multi-Head Self-Attention Mechanism.\n",
    "        :param h: the number of heads. Default: 8\n",
    "        :param d_model: the size of each embedding vector. Default: 512\n",
    "        :param dropout: probability of an element to be zeroed. Default: 0.1\n",
    "        \"\"\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        Multi-Head Self-Attention Mechanism.\n",
    "        :param query: (batch_size, src/tgt/tgt_seq_length, d_model)\n",
    "        :param key: (batch_size, src/tgt/src_seq_length, d_model)\n",
    "        :param value: (batch_size, src/tgt/src_seq_length, d_model)\n",
    "        :param mask: (batch_size, 1/tgt/1_seq_length, src/tgt/src_seq_length)\n",
    "        :return: (batch_size, src/tgt/tgt_seq_length, d_model)\n",
    "        \"\"\"\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        query, key, value = [\n",
    "            lin(x).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
    "            for lin, x in zip(self.linears, (query, key, value))\n",
    "        ]\n",
    "\n",
    "        x, self.attn = attention(\n",
    "            query, key, value, mask=mask, dropout=self.dropout\n",
    "        )\n",
    "\n",
    "        x = (\n",
    "            x.transpose(1, 2)\n",
    "            .contiguous()\n",
    "            .view(batch_size, -1, self.h * self.d_k)\n",
    "        )\n",
    "\n",
    "        del query\n",
    "        del key\n",
    "        del value\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model=512, d_ff=2048, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Position-wise Fully Connected Feed-Forward Network.\n",
    "        :param d_model: the size of each embedding vector. Default: 512\n",
    "        :param d_ff: dimension of the inner layer. Default: 2048\n",
    "        :param dropout: probability of an element to be zeroed. Default: 0.1\n",
    "        \"\"\"\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Position-wise Fully Connected Feed-Forward Network.\n",
    "        :param x: (batch_size, seq_length, d_model)\n",
    "        :return: (batch_size, seq_length, d_model)\n",
    "        \"\"\"\n",
    "        return self.w_2(self.dropout(self.w_1(x).relu()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sublayer Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    def __init__(self, d_model=512, dropout=0.1):\n",
    "        \"\"\"\n",
    "        We employ a residual connection around each of the two sub-layers,\n",
    "        followed by layer normalization (Actually we use pre-layer norm).\n",
    "        :param d_model: the size of each embedding vector. Default: 512\n",
    "        :param dropout: probability of an element to be zeroed. Default: 0.1\n",
    "        \"\"\"\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"\"\"\n",
    "        We employ a residual connection around each of the two sub-layers,\n",
    "        followed by layer normalization (Actually we use pre-layer norm).\n",
    "        :param x: (batch_size, seq_length, d_model)\n",
    "        :param sublayer: attention or feed-forward network.\n",
    "        :return: (batch_size, seq_length, d_model)\n",
    "        \"\"\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            self_attn: MultiHeadedAttention,\n",
    "            feed_forward: PositionwiseFeedForward,\n",
    "            d_model=512,\n",
    "            dropout=0.1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Identical Encoder Layer.\n",
    "        :param self_attn: multi-head self-attention.\n",
    "        :param feed_forward: position-wise fully connected feed-forward network.\n",
    "        :param d_model: the size of each embedding vector. Default: 512\n",
    "        :param dropout: probability of an element to be zeroed. Default: 0.1\n",
    "        \"\"\"\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayers = clones(SublayerConnection(d_model, dropout), 2)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        Identical Encoder Layer.\n",
    "        :param x: (batch_size, src_seq_length, d_model)\n",
    "        :param mask: (batch_size, 1, src_seq_length)\n",
    "        :return: (batch_size, src_seq_length, d_model)\n",
    "        \"\"\"\n",
    "        x = self.sublayers[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayers[1](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            self_attn: MultiHeadedAttention,\n",
    "            src_attn: MultiHeadedAttention,\n",
    "            feed_forward: PositionwiseFeedForward,\n",
    "            d_model=512,\n",
    "            dropout=0.1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Identical Decoder Layer.\n",
    "        :param self_attn: multi-head self-attention.\n",
    "        :param src_attn: encoder-decoder attention.\n",
    "        :param feed_forward: position-wise fully connected feed-forward network.\n",
    "        :param d_model: the size of each embedding vector. Default: 512\n",
    "        :param dropout: probability of an element to be zeroed. Default: 0.1\n",
    "        \"\"\"\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayers = clones(SublayerConnection(d_model, dropout), 3)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        \"\"\"\n",
    "        Identical Decoder Layer.\n",
    "        :param x: (batch_size, tgt_seq_length, d_model)\n",
    "        :param memory: (batch_size, src_seq_length, d_model)\n",
    "        :param src_mask: (batch_size, 1, src_seq_length)\n",
    "        :param tgt_mask: (batch_size, tgt_seq_length, tgt_seq_length)\n",
    "        :return: (batch_size, tgt_seq_length, d_model)\n",
    "        \"\"\"\n",
    "        m = memory\n",
    "        x = self.sublayers[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        x = self.sublayers[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
    "        return self.sublayers[2](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layer: EncoderLayer, n_layers=6):\n",
    "        \"\"\"\n",
    "        Encoder Stack.\n",
    "        :param layer: identical encoder layer.\n",
    "        :param n_layers: the number of encoder layers. Default: 6\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, n_layers)\n",
    "        self.norm = LayerNorm(layer.d_model)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        Encoder Stack.\n",
    "        :param x: (batch_size, src_seq_length, d_model)\n",
    "        :param mask: (batch_size, 1, src_seq_length)\n",
    "        :return: (batch_size, src_seq_length, d_model)\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layer: DecoderLayer, n_layers=6):\n",
    "        \"\"\"\n",
    "        Decoder Stack.\n",
    "        :param layer: identical decoder layer.\n",
    "        :param n_layers: the number of decoder layers. Default: 6\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = clones(layer, n_layers)\n",
    "        self.norm = LayerNorm(layer.d_model)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        \"\"\"\n",
    "        Decoder Stack.\n",
    "        :param x: (batch_size, tgt_seq_length, d_model)\n",
    "        :param memory: (batch_size, src_seq_length, d_model)\n",
    "        :param src_mask: (batch_size, 1, src_seq_length)\n",
    "        :param tgt_mask: (batch_size, tgt_seq_length, tgt_seq_length)\n",
    "        :return: (batch_size, tgt_seq_length, d_model)\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, vocab, d_model=512):\n",
    "        \"\"\"\n",
    "        Generator of Log Probabilities.\n",
    "        :param vocab: dictionary size of the target vocabulary.\n",
    "        :param d_model: the size of each embedding vector. Default: 512\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Generator of Log Probabilities.\n",
    "        :param x: (batch_size, tgt_seq_length, d_model)\n",
    "        :return: (batch_size, tgt_seq_length, vocab)\n",
    "        \"\"\"\n",
    "        return self.softmax(self.proj(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            encoder: Encoder,\n",
    "            decoder: Decoder,\n",
    "            src_embed: nn.Sequential,\n",
    "            tgt_embed: nn.Sequential,\n",
    "            generator: Generator,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Encoder-Decoder Architecture.\n",
    "        :param encoder: encoder stack.\n",
    "        :param decoder: decoder stack.\n",
    "        :param src_embed: sequence of embedding and positional encoding.\n",
    "        :param tgt_embed: sequence of embedding and positional encoding.\n",
    "        :param generator: generator of log probabilities.\n",
    "        \"\"\"\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        \"\"\"\n",
    "        Encoder-Decoder Architecture.\n",
    "        :param src: (batch_size, src_seq_length)\n",
    "        :param tgt: (batch_size, tgt_seq_length)\n",
    "        :param src_mask: (batch_size, 1, src_seq_length)\n",
    "        :param tgt_mask: (batch_size, tgt_seq_length, tgt_seq_length)\n",
    "        :return: (batch_size, tgt_seq_length)\n",
    "        \"\"\"\n",
    "        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        \"\"\"\n",
    "        Encoding Process.\n",
    "        :param src: (batch_size, src_seq_length)\n",
    "        :param src_mask: (batch_size, 1, src_seq_length)\n",
    "        :return: (batch_size, src_seq_length, d_model)\n",
    "        \"\"\"\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "\n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        \"\"\"\n",
    "        Encoding Process.\n",
    "        :param memory: (batch_size, src_seq_length, d_model)\n",
    "        :param src_mask: (batch_size, 1, src_seq_length)\n",
    "        :param tgt: (batch_size, tgt_seq_length)\n",
    "        :param tgt_mask: (batch_size, tgt_seq_length, tgt_seq_length)\n",
    "        :return: (batch_size, tgt_seq_length, d_model)\n",
    "        \"\"\"\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(\n",
    "        src_vocab,\n",
    "        tgt_vocab,\n",
    "        n_layers=6,\n",
    "        d_model=512,\n",
    "        d_ff=2048,\n",
    "        n_heads=8,\n",
    "        dropout=0.1,\n",
    "        max_len=5000,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a Transformer Block.\n",
    "    :param src_vocab: dictionary size of the source vocabulary.\n",
    "    :param tgt_vocab: dictionary size of the target vocabulary.\n",
    "    :param n_layers: the number of decoder layers. Default: 6\n",
    "    :param d_model: the size of each embedding vector. Default: 512\n",
    "    :param d_ff: dimension of the inner layer. Default: 2048\n",
    "    :param n_heads: the number of heads. Default: 8\n",
    "    :param dropout: probability of an element to be zeroed. Default: 0.1\n",
    "    :param max_len: max length of the sequence. Default: 5000\n",
    "    :return: transformer encoder-decoder architecture.\n",
    "    \"\"\"\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(n_heads, d_model, dropout)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout, max_len)\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(c(attn), c(ff), d_model, dropout), n_layers),\n",
    "        Decoder(DecoderLayer(c(attn), c(attn), c(ff), d_model, dropout), n_layers),\n",
    "        nn.Sequential(Embedding(src_vocab, d_model), c(position)),\n",
    "        nn.Sequential(Embedding(tgt_vocab, d_model), c(position)),\n",
    "        Generator(tgt_vocab, d_model),\n",
    "    )\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"\"\"\n",
    "    Scaled Dot-Product Attention.\n",
    "    :param query: (batch_size, head_num, src/tgt/tgt_seq_length, d_k)\n",
    "    :param key: (batch_size, head_num, src/tgt/src_seq_length, d_k)\n",
    "    :param value: (batch_size, head_num, src/tgt/src_seq_length, d_k)\n",
    "    :param mask: (batch_size, 1, 1/tgt/1_seq_length, src/tgt/src_seq_length)\n",
    "    :param dropout: probability of an element to be zeroed. Default: None\n",
    "    :return: (batch_size, head_num, src/tgt/tgt_seq_length, d_k), (batch_size, head_num, src/tgt/tgt_seq_length, src/tgt/src_seq_length)\n",
    "    \"\"\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = scores.softmax(dim=-1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(\n",
    "        torch.uint8\n",
    "    )\n",
    "    return subsequent_mask == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "vocab = 25000\n",
    "batch_size = 1\n",
    "seq_length = 500\n",
    "\n",
    "data = torch.randint(1, vocab, size=(batch_size, seq_length)).to(device)\n",
    "data[:, 0] = 1\n",
    "src = data.clone()\n",
    "tgt = data.clone()\n",
    "src_mask = (src != 0).unsqueeze(-2)\n",
    "tgt = tgt[:, :-1]\n",
    "tgt_mask = (tgt != 0).unsqueeze(-2)\n",
    "tgt_mask = tgt_mask & subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer (base model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------\n",
      "Layer (type)                            Output Shape              Param #\n",
      "==========================================================================\n",
      "Embedding-1                           [-1, 500, 512]           12,800,000\n",
      "Dropout-2                             [-1, 500, 512]                    0\n",
      "LayerNorm-3                           [-1, 500, 512]                1,024\n",
      "Linear-4                              [-1, 500, 512]              262,656\n",
      "Linear-5                              [-1, 500, 512]              262,656\n",
      "Linear-6                              [-1, 500, 512]              262,656\n",
      "Dropout-7                          [-1, 8, 500, 500]                    0\n",
      "Linear-8                              [-1, 500, 512]              262,656\n",
      "Dropout-9                             [-1, 500, 512]                    0\n",
      "LayerNorm-10                          [-1, 500, 512]                1,024\n",
      "Linear-11                            [-1, 500, 2048]            1,050,624\n",
      "Dropout-12                           [-1, 500, 2048]                    0\n",
      "Linear-13                             [-1, 500, 512]            1,049,088\n",
      "Dropout-14                            [-1, 500, 512]                    0\n",
      "LayerNorm-15                          [-1, 500, 512]                1,024\n",
      "Linear-16                             [-1, 500, 512]              262,656\n",
      "Linear-17                             [-1, 500, 512]              262,656\n",
      "Linear-18                             [-1, 500, 512]              262,656\n",
      "Dropout-19                         [-1, 8, 500, 500]                    0\n",
      "Linear-20                             [-1, 500, 512]              262,656\n",
      "Dropout-21                            [-1, 500, 512]                    0\n",
      "LayerNorm-22                          [-1, 500, 512]                1,024\n",
      "Linear-23                            [-1, 500, 2048]            1,050,624\n",
      "Dropout-24                           [-1, 500, 2048]                    0\n",
      "Linear-25                             [-1, 500, 512]            1,049,088\n",
      "Dropout-26                            [-1, 500, 512]                    0\n",
      "LayerNorm-27                          [-1, 500, 512]                1,024\n",
      "Linear-28                             [-1, 500, 512]              262,656\n",
      "Linear-29                             [-1, 500, 512]              262,656\n",
      "Linear-30                             [-1, 500, 512]              262,656\n",
      "Dropout-31                         [-1, 8, 500, 500]                    0\n",
      "Linear-32                             [-1, 500, 512]              262,656\n",
      "Dropout-33                            [-1, 500, 512]                    0\n",
      "LayerNorm-34                          [-1, 500, 512]                1,024\n",
      "Linear-35                            [-1, 500, 2048]            1,050,624\n",
      "Dropout-36                           [-1, 500, 2048]                    0\n",
      "Linear-37                             [-1, 500, 512]            1,049,088\n",
      "Dropout-38                            [-1, 500, 512]                    0\n",
      "LayerNorm-39                          [-1, 500, 512]                1,024\n",
      "Linear-40                             [-1, 500, 512]              262,656\n",
      "Linear-41                             [-1, 500, 512]              262,656\n",
      "Linear-42                             [-1, 500, 512]              262,656\n",
      "Dropout-43                         [-1, 8, 500, 500]                    0\n",
      "Linear-44                             [-1, 500, 512]              262,656\n",
      "Dropout-45                            [-1, 500, 512]                    0\n",
      "LayerNorm-46                          [-1, 500, 512]                1,024\n",
      "Linear-47                            [-1, 500, 2048]            1,050,624\n",
      "Dropout-48                           [-1, 500, 2048]                    0\n",
      "Linear-49                             [-1, 500, 512]            1,049,088\n",
      "Dropout-50                            [-1, 500, 512]                    0\n",
      "LayerNorm-51                          [-1, 500, 512]                1,024\n",
      "Linear-52                             [-1, 500, 512]              262,656\n",
      "Linear-53                             [-1, 500, 512]              262,656\n",
      "Linear-54                             [-1, 500, 512]              262,656\n",
      "Dropout-55                         [-1, 8, 500, 500]                    0\n",
      "Linear-56                             [-1, 500, 512]              262,656\n",
      "Dropout-57                            [-1, 500, 512]                    0\n",
      "LayerNorm-58                          [-1, 500, 512]                1,024\n",
      "Linear-59                            [-1, 500, 2048]            1,050,624\n",
      "Dropout-60                           [-1, 500, 2048]                    0\n",
      "Linear-61                             [-1, 500, 512]            1,049,088\n",
      "Dropout-62                            [-1, 500, 512]                    0\n",
      "LayerNorm-63                          [-1, 500, 512]                1,024\n",
      "Linear-64                             [-1, 500, 512]              262,656\n",
      "Linear-65                             [-1, 500, 512]              262,656\n",
      "Linear-66                             [-1, 500, 512]              262,656\n",
      "Dropout-67                         [-1, 8, 500, 500]                    0\n",
      "Linear-68                             [-1, 500, 512]              262,656\n",
      "Dropout-69                            [-1, 500, 512]                    0\n",
      "LayerNorm-70                          [-1, 500, 512]                1,024\n",
      "Linear-71                            [-1, 500, 2048]            1,050,624\n",
      "Dropout-72                           [-1, 500, 2048]                    0\n",
      "Linear-73                             [-1, 500, 512]            1,049,088\n",
      "Dropout-74                            [-1, 500, 512]                    0\n",
      "LayerNorm-75                          [-1, 500, 512]                1,024\n",
      "Embedding-76                          [-1, 499, 512]           12,800,000\n",
      "Dropout-77                            [-1, 499, 512]                    0\n",
      "LayerNorm-78                          [-1, 499, 512]                1,024\n",
      "Linear-79                             [-1, 499, 512]              262,656\n",
      "Linear-80                             [-1, 499, 512]              262,656\n",
      "Linear-81                             [-1, 499, 512]              262,656\n",
      "Dropout-82                         [-1, 8, 499, 499]                    0\n",
      "Linear-83                             [-1, 499, 512]              262,656\n",
      "Dropout-84                            [-1, 499, 512]                    0\n",
      "LayerNorm-85                          [-1, 499, 512]                1,024\n",
      "Linear-86                             [-1, 499, 512]              262,656\n",
      "Linear-87                             [-1, 500, 512]              262,656\n",
      "Linear-88                             [-1, 500, 512]              262,656\n",
      "Dropout-89                         [-1, 8, 499, 500]                    0\n",
      "Linear-90                             [-1, 499, 512]              262,656\n",
      "Dropout-91                            [-1, 499, 512]                    0\n",
      "LayerNorm-92                          [-1, 499, 512]                1,024\n",
      "Linear-93                            [-1, 499, 2048]            1,050,624\n",
      "Dropout-94                           [-1, 499, 2048]                    0\n",
      "Linear-95                             [-1, 499, 512]            1,049,088\n",
      "Dropout-96                            [-1, 499, 512]                    0\n",
      "LayerNorm-97                          [-1, 499, 512]                1,024\n",
      "Linear-98                             [-1, 499, 512]              262,656\n",
      "Linear-99                             [-1, 499, 512]              262,656\n",
      "Linear-100                            [-1, 499, 512]              262,656\n",
      "Dropout-101                        [-1, 8, 499, 499]                    0\n",
      "Linear-102                            [-1, 499, 512]              262,656\n",
      "Dropout-103                           [-1, 499, 512]                    0\n",
      "LayerNorm-104                         [-1, 499, 512]                1,024\n",
      "Linear-105                            [-1, 499, 512]              262,656\n",
      "Linear-106                            [-1, 500, 512]              262,656\n",
      "Linear-107                            [-1, 500, 512]              262,656\n",
      "Dropout-108                        [-1, 8, 499, 500]                    0\n",
      "Linear-109                            [-1, 499, 512]              262,656\n",
      "Dropout-110                           [-1, 499, 512]                    0\n",
      "LayerNorm-111                         [-1, 499, 512]                1,024\n",
      "Linear-112                           [-1, 499, 2048]            1,050,624\n",
      "Dropout-113                          [-1, 499, 2048]                    0\n",
      "Linear-114                            [-1, 499, 512]            1,049,088\n",
      "Dropout-115                           [-1, 499, 512]                    0\n",
      "LayerNorm-116                         [-1, 499, 512]                1,024\n",
      "Linear-117                            [-1, 499, 512]              262,656\n",
      "Linear-118                            [-1, 499, 512]              262,656\n",
      "Linear-119                            [-1, 499, 512]              262,656\n",
      "Dropout-120                        [-1, 8, 499, 499]                    0\n",
      "Linear-121                            [-1, 499, 512]              262,656\n",
      "Dropout-122                           [-1, 499, 512]                    0\n",
      "LayerNorm-123                         [-1, 499, 512]                1,024\n",
      "Linear-124                            [-1, 499, 512]              262,656\n",
      "Linear-125                            [-1, 500, 512]              262,656\n",
      "Linear-126                            [-1, 500, 512]              262,656\n",
      "Dropout-127                        [-1, 8, 499, 500]                    0\n",
      "Linear-128                            [-1, 499, 512]              262,656\n",
      "Dropout-129                           [-1, 499, 512]                    0\n",
      "LayerNorm-130                         [-1, 499, 512]                1,024\n",
      "Linear-131                           [-1, 499, 2048]            1,050,624\n",
      "Dropout-132                          [-1, 499, 2048]                    0\n",
      "Linear-133                            [-1, 499, 512]            1,049,088\n",
      "Dropout-134                           [-1, 499, 512]                    0\n",
      "LayerNorm-135                         [-1, 499, 512]                1,024\n",
      "Linear-136                            [-1, 499, 512]              262,656\n",
      "Linear-137                            [-1, 499, 512]              262,656\n",
      "Linear-138                            [-1, 499, 512]              262,656\n",
      "Dropout-139                        [-1, 8, 499, 499]                    0\n",
      "Linear-140                            [-1, 499, 512]              262,656\n",
      "Dropout-141                           [-1, 499, 512]                    0\n",
      "LayerNorm-142                         [-1, 499, 512]                1,024\n",
      "Linear-143                            [-1, 499, 512]              262,656\n",
      "Linear-144                            [-1, 500, 512]              262,656\n",
      "Linear-145                            [-1, 500, 512]              262,656\n",
      "Dropout-146                        [-1, 8, 499, 500]                    0\n",
      "Linear-147                            [-1, 499, 512]              262,656\n",
      "Dropout-148                           [-1, 499, 512]                    0\n",
      "LayerNorm-149                         [-1, 499, 512]                1,024\n",
      "Linear-150                           [-1, 499, 2048]            1,050,624\n",
      "Dropout-151                          [-1, 499, 2048]                    0\n",
      "Linear-152                            [-1, 499, 512]            1,049,088\n",
      "Dropout-153                           [-1, 499, 512]                    0\n",
      "LayerNorm-154                         [-1, 499, 512]                1,024\n",
      "Linear-155                            [-1, 499, 512]              262,656\n",
      "Linear-156                            [-1, 499, 512]              262,656\n",
      "Linear-157                            [-1, 499, 512]              262,656\n",
      "Dropout-158                        [-1, 8, 499, 499]                    0\n",
      "Linear-159                            [-1, 499, 512]              262,656\n",
      "Dropout-160                           [-1, 499, 512]                    0\n",
      "LayerNorm-161                         [-1, 499, 512]                1,024\n",
      "Linear-162                            [-1, 499, 512]              262,656\n",
      "Linear-163                            [-1, 500, 512]              262,656\n",
      "Linear-164                            [-1, 500, 512]              262,656\n",
      "Dropout-165                        [-1, 8, 499, 500]                    0\n",
      "Linear-166                            [-1, 499, 512]              262,656\n",
      "Dropout-167                           [-1, 499, 512]                    0\n",
      "LayerNorm-168                         [-1, 499, 512]                1,024\n",
      "Linear-169                           [-1, 499, 2048]            1,050,624\n",
      "Dropout-170                          [-1, 499, 2048]                    0\n",
      "Linear-171                            [-1, 499, 512]            1,049,088\n",
      "Dropout-172                           [-1, 499, 512]                    0\n",
      "LayerNorm-173                         [-1, 499, 512]                1,024\n",
      "Linear-174                            [-1, 499, 512]              262,656\n",
      "Linear-175                            [-1, 499, 512]              262,656\n",
      "Linear-176                            [-1, 499, 512]              262,656\n",
      "Dropout-177                        [-1, 8, 499, 499]                    0\n",
      "Linear-178                            [-1, 499, 512]              262,656\n",
      "Dropout-179                           [-1, 499, 512]                    0\n",
      "LayerNorm-180                         [-1, 499, 512]                1,024\n",
      "Linear-181                            [-1, 499, 512]              262,656\n",
      "Linear-182                            [-1, 500, 512]              262,656\n",
      "Linear-183                            [-1, 500, 512]              262,656\n",
      "Dropout-184                        [-1, 8, 499, 500]                    0\n",
      "Linear-185                            [-1, 499, 512]              262,656\n",
      "Dropout-186                           [-1, 499, 512]                    0\n",
      "LayerNorm-187                         [-1, 499, 512]                1,024\n",
      "Linear-188                           [-1, 499, 2048]            1,050,624\n",
      "Dropout-189                          [-1, 499, 2048]                    0\n",
      "Linear-190                            [-1, 499, 512]            1,049,088\n",
      "Dropout-191                           [-1, 499, 512]                    0\n",
      "LayerNorm-192                         [-1, 499, 512]                1,024\n",
      "==========================================================================\n",
      "Total params: 69,740,544\n",
      "Trainable params: 69,740,544\n",
      "Non-trainable params: 0\n",
      "--------------------------------------------------------------------------\n",
      "Input size (MB): 0.000000\n",
      "Forward/backward pass size (MB): 754.074097\n",
      "Params size (MB): 266.039062\n",
      "Estimated Total Size (MB): 1020.113159\n",
      "--------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchkeras import summary\n",
    "\n",
    "net = transformer(\n",
    "    src_vocab=vocab,\n",
    "    tgt_vocab=vocab,\n",
    "    n_layers=6,\n",
    "    d_model=512,\n",
    "    d_ff=2048,\n",
    "    n_heads=8,\n",
    "    dropout=0.1,\n",
    "    max_len=5000,\n",
    ").to(device)\n",
    "\n",
    "summary(net, input_data_args=[src, tgt, src_mask, tgt_mask]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer (big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------\n",
      "Layer (type)                            Output Shape              Param #\n",
      "==========================================================================\n",
      "Embedding-1                          [-1, 500, 1024]           25,600,000\n",
      "Dropout-2                            [-1, 500, 1024]                    0\n",
      "LayerNorm-3                          [-1, 500, 1024]                2,048\n",
      "Linear-4                             [-1, 500, 1024]            1,049,600\n",
      "Linear-5                             [-1, 500, 1024]            1,049,600\n",
      "Linear-6                             [-1, 500, 1024]            1,049,600\n",
      "Dropout-7                         [-1, 16, 500, 500]                    0\n",
      "Linear-8                             [-1, 500, 1024]            1,049,600\n",
      "Dropout-9                            [-1, 500, 1024]                    0\n",
      "LayerNorm-10                         [-1, 500, 1024]                2,048\n",
      "Linear-11                            [-1, 500, 4096]            4,198,400\n",
      "Dropout-12                           [-1, 500, 4096]                    0\n",
      "Linear-13                            [-1, 500, 1024]            4,195,328\n",
      "Dropout-14                           [-1, 500, 1024]                    0\n",
      "LayerNorm-15                         [-1, 500, 1024]                2,048\n",
      "Linear-16                            [-1, 500, 1024]            1,049,600\n",
      "Linear-17                            [-1, 500, 1024]            1,049,600\n",
      "Linear-18                            [-1, 500, 1024]            1,049,600\n",
      "Dropout-19                        [-1, 16, 500, 500]                    0\n",
      "Linear-20                            [-1, 500, 1024]            1,049,600\n",
      "Dropout-21                           [-1, 500, 1024]                    0\n",
      "LayerNorm-22                         [-1, 500, 1024]                2,048\n",
      "Linear-23                            [-1, 500, 4096]            4,198,400\n",
      "Dropout-24                           [-1, 500, 4096]                    0\n",
      "Linear-25                            [-1, 500, 1024]            4,195,328\n",
      "Dropout-26                           [-1, 500, 1024]                    0\n",
      "LayerNorm-27                         [-1, 500, 1024]                2,048\n",
      "Linear-28                            [-1, 500, 1024]            1,049,600\n",
      "Linear-29                            [-1, 500, 1024]            1,049,600\n",
      "Linear-30                            [-1, 500, 1024]            1,049,600\n",
      "Dropout-31                        [-1, 16, 500, 500]                    0\n",
      "Linear-32                            [-1, 500, 1024]            1,049,600\n",
      "Dropout-33                           [-1, 500, 1024]                    0\n",
      "LayerNorm-34                         [-1, 500, 1024]                2,048\n",
      "Linear-35                            [-1, 500, 4096]            4,198,400\n",
      "Dropout-36                           [-1, 500, 4096]                    0\n",
      "Linear-37                            [-1, 500, 1024]            4,195,328\n",
      "Dropout-38                           [-1, 500, 1024]                    0\n",
      "LayerNorm-39                         [-1, 500, 1024]                2,048\n",
      "Linear-40                            [-1, 500, 1024]            1,049,600\n",
      "Linear-41                            [-1, 500, 1024]            1,049,600\n",
      "Linear-42                            [-1, 500, 1024]            1,049,600\n",
      "Dropout-43                        [-1, 16, 500, 500]                    0\n",
      "Linear-44                            [-1, 500, 1024]            1,049,600\n",
      "Dropout-45                           [-1, 500, 1024]                    0\n",
      "LayerNorm-46                         [-1, 500, 1024]                2,048\n",
      "Linear-47                            [-1, 500, 4096]            4,198,400\n",
      "Dropout-48                           [-1, 500, 4096]                    0\n",
      "Linear-49                            [-1, 500, 1024]            4,195,328\n",
      "Dropout-50                           [-1, 500, 1024]                    0\n",
      "LayerNorm-51                         [-1, 500, 1024]                2,048\n",
      "Linear-52                            [-1, 500, 1024]            1,049,600\n",
      "Linear-53                            [-1, 500, 1024]            1,049,600\n",
      "Linear-54                            [-1, 500, 1024]            1,049,600\n",
      "Dropout-55                        [-1, 16, 500, 500]                    0\n",
      "Linear-56                            [-1, 500, 1024]            1,049,600\n",
      "Dropout-57                           [-1, 500, 1024]                    0\n",
      "LayerNorm-58                         [-1, 500, 1024]                2,048\n",
      "Linear-59                            [-1, 500, 4096]            4,198,400\n",
      "Dropout-60                           [-1, 500, 4096]                    0\n",
      "Linear-61                            [-1, 500, 1024]            4,195,328\n",
      "Dropout-62                           [-1, 500, 1024]                    0\n",
      "LayerNorm-63                         [-1, 500, 1024]                2,048\n",
      "Linear-64                            [-1, 500, 1024]            1,049,600\n",
      "Linear-65                            [-1, 500, 1024]            1,049,600\n",
      "Linear-66                            [-1, 500, 1024]            1,049,600\n",
      "Dropout-67                        [-1, 16, 500, 500]                    0\n",
      "Linear-68                            [-1, 500, 1024]            1,049,600\n",
      "Dropout-69                           [-1, 500, 1024]                    0\n",
      "LayerNorm-70                         [-1, 500, 1024]                2,048\n",
      "Linear-71                            [-1, 500, 4096]            4,198,400\n",
      "Dropout-72                           [-1, 500, 4096]                    0\n",
      "Linear-73                            [-1, 500, 1024]            4,195,328\n",
      "Dropout-74                           [-1, 500, 1024]                    0\n",
      "LayerNorm-75                         [-1, 500, 1024]                2,048\n",
      "Embedding-76                         [-1, 499, 1024]           25,600,000\n",
      "Dropout-77                           [-1, 499, 1024]                    0\n",
      "LayerNorm-78                         [-1, 499, 1024]                2,048\n",
      "Linear-79                            [-1, 499, 1024]            1,049,600\n",
      "Linear-80                            [-1, 499, 1024]            1,049,600\n",
      "Linear-81                            [-1, 499, 1024]            1,049,600\n",
      "Dropout-82                        [-1, 16, 499, 499]                    0\n",
      "Linear-83                            [-1, 499, 1024]            1,049,600\n",
      "Dropout-84                           [-1, 499, 1024]                    0\n",
      "LayerNorm-85                         [-1, 499, 1024]                2,048\n",
      "Linear-86                            [-1, 499, 1024]            1,049,600\n",
      "Linear-87                            [-1, 500, 1024]            1,049,600\n",
      "Linear-88                            [-1, 500, 1024]            1,049,600\n",
      "Dropout-89                        [-1, 16, 499, 500]                    0\n",
      "Linear-90                            [-1, 499, 1024]            1,049,600\n",
      "Dropout-91                           [-1, 499, 1024]                    0\n",
      "LayerNorm-92                         [-1, 499, 1024]                2,048\n",
      "Linear-93                            [-1, 499, 4096]            4,198,400\n",
      "Dropout-94                           [-1, 499, 4096]                    0\n",
      "Linear-95                            [-1, 499, 1024]            4,195,328\n",
      "Dropout-96                           [-1, 499, 1024]                    0\n",
      "LayerNorm-97                         [-1, 499, 1024]                2,048\n",
      "Linear-98                            [-1, 499, 1024]            1,049,600\n",
      "Linear-99                            [-1, 499, 1024]            1,049,600\n",
      "Linear-100                           [-1, 499, 1024]            1,049,600\n",
      "Dropout-101                       [-1, 16, 499, 499]                    0\n",
      "Linear-102                           [-1, 499, 1024]            1,049,600\n",
      "Dropout-103                          [-1, 499, 1024]                    0\n",
      "LayerNorm-104                        [-1, 499, 1024]                2,048\n",
      "Linear-105                           [-1, 499, 1024]            1,049,600\n",
      "Linear-106                           [-1, 500, 1024]            1,049,600\n",
      "Linear-107                           [-1, 500, 1024]            1,049,600\n",
      "Dropout-108                       [-1, 16, 499, 500]                    0\n",
      "Linear-109                           [-1, 499, 1024]            1,049,600\n",
      "Dropout-110                          [-1, 499, 1024]                    0\n",
      "LayerNorm-111                        [-1, 499, 1024]                2,048\n",
      "Linear-112                           [-1, 499, 4096]            4,198,400\n",
      "Dropout-113                          [-1, 499, 4096]                    0\n",
      "Linear-114                           [-1, 499, 1024]            4,195,328\n",
      "Dropout-115                          [-1, 499, 1024]                    0\n",
      "LayerNorm-116                        [-1, 499, 1024]                2,048\n",
      "Linear-117                           [-1, 499, 1024]            1,049,600\n",
      "Linear-118                           [-1, 499, 1024]            1,049,600\n",
      "Linear-119                           [-1, 499, 1024]            1,049,600\n",
      "Dropout-120                       [-1, 16, 499, 499]                    0\n",
      "Linear-121                           [-1, 499, 1024]            1,049,600\n",
      "Dropout-122                          [-1, 499, 1024]                    0\n",
      "LayerNorm-123                        [-1, 499, 1024]                2,048\n",
      "Linear-124                           [-1, 499, 1024]            1,049,600\n",
      "Linear-125                           [-1, 500, 1024]            1,049,600\n",
      "Linear-126                           [-1, 500, 1024]            1,049,600\n",
      "Dropout-127                       [-1, 16, 499, 500]                    0\n",
      "Linear-128                           [-1, 499, 1024]            1,049,600\n",
      "Dropout-129                          [-1, 499, 1024]                    0\n",
      "LayerNorm-130                        [-1, 499, 1024]                2,048\n",
      "Linear-131                           [-1, 499, 4096]            4,198,400\n",
      "Dropout-132                          [-1, 499, 4096]                    0\n",
      "Linear-133                           [-1, 499, 1024]            4,195,328\n",
      "Dropout-134                          [-1, 499, 1024]                    0\n",
      "LayerNorm-135                        [-1, 499, 1024]                2,048\n",
      "Linear-136                           [-1, 499, 1024]            1,049,600\n",
      "Linear-137                           [-1, 499, 1024]            1,049,600\n",
      "Linear-138                           [-1, 499, 1024]            1,049,600\n",
      "Dropout-139                       [-1, 16, 499, 499]                    0\n",
      "Linear-140                           [-1, 499, 1024]            1,049,600\n",
      "Dropout-141                          [-1, 499, 1024]                    0\n",
      "LayerNorm-142                        [-1, 499, 1024]                2,048\n",
      "Linear-143                           [-1, 499, 1024]            1,049,600\n",
      "Linear-144                           [-1, 500, 1024]            1,049,600\n",
      "Linear-145                           [-1, 500, 1024]            1,049,600\n",
      "Dropout-146                       [-1, 16, 499, 500]                    0\n",
      "Linear-147                           [-1, 499, 1024]            1,049,600\n",
      "Dropout-148                          [-1, 499, 1024]                    0\n",
      "LayerNorm-149                        [-1, 499, 1024]                2,048\n",
      "Linear-150                           [-1, 499, 4096]            4,198,400\n",
      "Dropout-151                          [-1, 499, 4096]                    0\n",
      "Linear-152                           [-1, 499, 1024]            4,195,328\n",
      "Dropout-153                          [-1, 499, 1024]                    0\n",
      "LayerNorm-154                        [-1, 499, 1024]                2,048\n",
      "Linear-155                           [-1, 499, 1024]            1,049,600\n",
      "Linear-156                           [-1, 499, 1024]            1,049,600\n",
      "Linear-157                           [-1, 499, 1024]            1,049,600\n",
      "Dropout-158                       [-1, 16, 499, 499]                    0\n",
      "Linear-159                           [-1, 499, 1024]            1,049,600\n",
      "Dropout-160                          [-1, 499, 1024]                    0\n",
      "LayerNorm-161                        [-1, 499, 1024]                2,048\n",
      "Linear-162                           [-1, 499, 1024]            1,049,600\n",
      "Linear-163                           [-1, 500, 1024]            1,049,600\n",
      "Linear-164                           [-1, 500, 1024]            1,049,600\n",
      "Dropout-165                       [-1, 16, 499, 500]                    0\n",
      "Linear-166                           [-1, 499, 1024]            1,049,600\n",
      "Dropout-167                          [-1, 499, 1024]                    0\n",
      "LayerNorm-168                        [-1, 499, 1024]                2,048\n",
      "Linear-169                           [-1, 499, 4096]            4,198,400\n",
      "Dropout-170                          [-1, 499, 4096]                    0\n",
      "Linear-171                           [-1, 499, 1024]            4,195,328\n",
      "Dropout-172                          [-1, 499, 1024]                    0\n",
      "LayerNorm-173                        [-1, 499, 1024]                2,048\n",
      "Linear-174                           [-1, 499, 1024]            1,049,600\n",
      "Linear-175                           [-1, 499, 1024]            1,049,600\n",
      "Linear-176                           [-1, 499, 1024]            1,049,600\n",
      "Dropout-177                       [-1, 16, 499, 499]                    0\n",
      "Linear-178                           [-1, 499, 1024]            1,049,600\n",
      "Dropout-179                          [-1, 499, 1024]                    0\n",
      "LayerNorm-180                        [-1, 499, 1024]                2,048\n",
      "Linear-181                           [-1, 499, 1024]            1,049,600\n",
      "Linear-182                           [-1, 500, 1024]            1,049,600\n",
      "Linear-183                           [-1, 500, 1024]            1,049,600\n",
      "Dropout-184                       [-1, 16, 499, 500]                    0\n",
      "Linear-185                           [-1, 499, 1024]            1,049,600\n",
      "Dropout-186                          [-1, 499, 1024]                    0\n",
      "LayerNorm-187                        [-1, 499, 1024]                2,048\n",
      "Linear-188                           [-1, 499, 4096]            4,198,400\n",
      "Dropout-189                          [-1, 499, 4096]                    0\n",
      "Linear-190                           [-1, 499, 1024]            4,195,328\n",
      "Dropout-191                          [-1, 499, 1024]                    0\n",
      "LayerNorm-192                        [-1, 499, 1024]                2,048\n",
      "==========================================================================\n",
      "Total params: 227,561,472\n",
      "Trainable params: 227,561,472\n",
      "Non-trainable params: 0\n",
      "--------------------------------------------------------------------------\n",
      "Input size (MB): 0.000000\n",
      "Forward/backward pass size (MB): 1508.148193\n",
      "Params size (MB): 868.078125\n",
      "Estimated Total Size (MB): 2376.226318\n",
      "--------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "net = transformer(\n",
    "    src_vocab=vocab,\n",
    "    tgt_vocab=vocab,\n",
    "    n_layers=6,\n",
    "    d_model=1024,\n",
    "    d_ff=4096,\n",
    "    n_heads=16,\n",
    "    dropout=0.3,\n",
    "    max_len=5000,\n",
    ").to(device)\n",
    "\n",
    "summary(net, input_data_args=[src, tgt, src_mask, tgt_mask]);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
